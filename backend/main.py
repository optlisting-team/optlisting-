from fastapi import FastAPI, Depends, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import Response, JSONResponse
from fastapi.exceptions import RequestValidationError
from starlette.exceptions import HTTPException as StarletteHTTPException
from starlette.requests import Request
from sqlalchemy.orm import Session
from sqlalchemy import func
from typing import Optional, List, Dict
from datetime import date, datetime, timedelta
import json
import logging
from pydantic import BaseModel

from .models import init_db, get_db, Listing, DeletionLog, Profile, CSVFormat, Base, engine
from .services import detect_source, extract_supplier_info, analyze_zombie_listings, generate_export_csv
from .dummy_data import generate_dummy_listings
from .webhooks import verify_webhook_signature, process_webhook_event
from .ebay_webhook import router as ebay_webhook_router
from .credit_service import (
    get_available_credits,
    check_credits,
    deduct_credits_atomic,
    add_credits,
    initialize_user_credits,
    get_credit_summary,
    refund_credits,
    CreditChecker,
    TransactionType,
    PlanType,
)

app = FastAPI(title="OptListing API", version="1.3.12")

# eBay Webhook Router ë“±ë¡
app.include_router(ebay_webhook_router)

# In-memory cache for KPI metrics (5-minute TTL)
# Structure: {cache_key: {"data": {...}, "timestamp": datetime}}
kpi_cache: Dict[str, Dict] = {}
CACHE_TTL_SECONDS = 300  # 5 minutes


def get_cache_key(user_id: str, store_id: Optional[str], marketplace: str, filters: Dict) -> str:
    """Generate a unique cache key for KPI metrics"""
    filter_str = "_".join(f"{k}={v}" for k, v in sorted(filters.items()))
    return f"{user_id}_{store_id or 'all'}_{marketplace}_{filter_str}"


def get_cached_kpi(cache_key: str) -> Optional[Dict]:
    """Get cached KPI data if not expired"""
    if cache_key in kpi_cache:
        cached = kpi_cache[cache_key]
        if (datetime.now() - cached["timestamp"]).seconds < CACHE_TTL_SECONDS:
            return cached["data"]
        else:
            del kpi_cache[cache_key]
    return None


def set_cached_kpi(cache_key: str, data: Dict):
    """Set KPI data in cache"""
    kpi_cache[cache_key] = {"data": data, "timestamp": datetime.now()}

# CORS middleware for React frontend
# Allow both local development and production frontend URLs
import os
import re

# Define the allowed exact origins (for production build)
# CRITICAL: Include all variations of production URL (with and without trailing slash)
allowed_origins = [
    # ğŸš¨ CUSTOM DOMAIN - CRITICAL FOR PRODUCTION
    "https://optlisting.com",
    "https://optlisting.com/",
    "https://www.optlisting.com",
    "https://www.optlisting.com/",
    
    # Production Vercel deployment - All variations (CRITICAL for CORS)
    "https://optlisting.vercel.app",
    "https://optlisting.vercel.app/",
    "https://www.optlisting.vercel.app",
    "https://www.optlisting.vercel.app/",
    # Actual deployed Vercel domain
    "https://optlisting-three.vercel.app",
    "https://optlisting-three.vercel.app/",
    "https://optlisting-1fev8br9z-optlistings-projects.vercel.app",
    "https://optlisting-1fev8br9z-optlistings-projects.vercel.app/",
    
    # Local development environments
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:3000",
]

# Add environment variable for additional frontend URLs if provided
frontend_url = os.getenv("FRONTEND_URL", "")
if frontend_url:
    allowed_origins.append(frontend_url)
    # Also add with trailing slash if not present
    if not frontend_url.endswith("/"):
        allowed_origins.append(f"{frontend_url}/")

# Filter out empty strings
allowed_origins = [origin for origin in allowed_origins if origin]

# Define regex pattern to cover all Vercel deploy previews (*.vercel.app)
# CRITICAL: This regex MUST cover all Vercel subdomains (production, preview, branch deployments)
# Pattern matches: https://*.vercel.app (any subdomain)
vercel_regex = r"https://.*\.vercel\.app"

# CORS configuration for Railway + Vercel deployment
# CRITICAL: Ensure all Vercel domains are explicitly allowed
# Add CORS middleware FIRST, before any routes

# Log CORS configuration for debugging
logging.info(f"ğŸŒ CORS Configuration:")
logging.info(f"   Allowed origins: {allowed_origins}")
logging.info(f"   Vercel regex: {vercel_regex}")

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,  # Explicit production and local URLs
    allow_origin_regex=vercel_regex,  # CRITICAL: Regex pattern for all Vercel subdomains
    allow_credentials=True,  # Enable credentials for authenticated requests
    allow_methods=["*"],  # Allow all HTTP methods
    allow_headers=["*"],  # Allow all headers
    expose_headers=["*"],  # Expose all headers
    max_age=3600,  # Cache preflight requests for 1 hour
)

# Exception handlers to ensure CORS headers are always present
@app.exception_handler(StarletteHTTPException)
async def http_exception_handler(request, exc):
    """Handle HTTP exceptions with CORS headers"""
    return JSONResponse(
        status_code=exc.status_code,
        content={"detail": exc.detail},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS, HEAD, PATCH",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request, exc):
    """Handle validation errors with CORS headers"""
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors()},
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS, HEAD, PATCH",
            "Access-Control-Allow-Headers": "*",
        }
    )

@app.exception_handler(Exception)
async def global_exception_handler(request, exc):
    """Handle all other exceptions and ensure CORS headers are present"""
    import traceback
    import logging
    
    logger = logging.getLogger(__name__)
    
    # ìƒì„¸í•œ ì—ëŸ¬ ë¡œê¹…
    error_traceback = traceback.format_exc()
    logger.error(f"âŒ Unhandled exception: {type(exc).__name__}: {str(exc)}")
    logger.error(f"   Request URL: {request.url}")
    logger.error(f"   Request method: {request.method}")
    logger.error(f"   Error traceback:\n{error_traceback}")
    
    # Return error response with CORS headers
    return JSONResponse(
        status_code=500,
        content={
            "detail": "Internal server error",
            "error_type": type(exc).__name__,
            "error_message": str(exc) if not isinstance(exc, Exception) else str(exc)
        },
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS, HEAD, PATCH",
            "Access-Control-Allow-Headers": "*",
        }
    )

# Initialize database on startup
@app.on_event("startup")
def startup_event():
    """
    Initialize database connection and create tables.
    This function must not crash the server if database connection fails.
    """
    try:
        # Test database connection first
        print("Testing database connection...")
        from .models import engine
        from sqlalchemy import text
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        print("Database connection successful")
        
        # Create tables if they don't exist (works for both SQLite and Supabase)
        print("Creating database tables if they don't exist...")
        Base.metadata.create_all(bind=engine)
        print("Database tables created/verified successfully")
        
        # Generate dummy data on first startup (non-blocking)
        try:
            db = next(get_db())
            try:
                count = db.query(Listing).count()
                if count == 0:
                    print("Generating 550 dummy listings (500 active, 50 zombies)... This may take a moment.")
                    generate_dummy_listings(db, count=550, user_id="default-user")
                    print("Dummy data generated successfully")
                else:
                    print(f"Database already contains {count} listings")
            except Exception as e:
                print(f"Error generating dummy data: {e}")
                import traceback
                traceback.print_exc()
            finally:
                db.close()
        except Exception as e:
            print(f"Warning: Could not generate dummy data: {e}")
            # Don't crash the server if dummy data generation fails
            import traceback
            traceback.print_exc()
    except Exception as e:
        # Log error but don't crash the server
        print(f"CRITICAL: Database connection failed: {e}")
        print("Server will continue to start, but database operations may fail.")
        import traceback
        traceback.print_exc()
        # Server should still start even if database connection fails


@app.get("/")
def root():
    return {"message": "OptListing API is running"}


@app.get("/api/health")
def health_check():
    """
    ì„œë¹„ìŠ¤ Health Check ì—”ë“œí¬ì¸íŠ¸
    - API ìƒíƒœ
    - DB ì—°ê²° ìƒíƒœ
    - eBay Worker ìƒíƒœ
    """
    from datetime import datetime
    
    health = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": app.version,
        "services": {
            "api": "ok",
            "database": "unknown",
            "ebay_worker": "unknown"
        }
    }
    
    # DB ì—°ê²° í…ŒìŠ¤íŠ¸
    try:
        from .models import engine
        from sqlalchemy import text
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        health["services"]["database"] = "ok"
    except Exception as e:
        health["services"]["database"] = f"error: {str(e)[:50]}"
        health["status"] = "degraded"
    
    # Worker ìƒíƒœ í™•ì¸ (import ì‹œë„)
    try:
        from .workers.ebay_token_worker import get_worker_status
        worker_status = get_worker_status()
        health["services"]["ebay_worker"] = {
            "status": "running" if worker_status.get("is_running") else "idle",
            "last_run": worker_status.get("last_run"),
            "total_refreshed": worker_status.get("total_refreshed", 0),
            "ebay_configured": worker_status.get("ebay_configured", False)
        }
    except ImportError:
        health["services"]["ebay_worker"] = "not_loaded"
    except Exception as e:
        health["services"]["ebay_worker"] = f"error: {str(e)[:50]}"
    
    return health


@app.post("/api/worker/trigger-refresh")
async def trigger_token_refresh(
    admin_key: str = None
):
    """
    ìˆ˜ë™ìœ¼ë¡œ eBay Token ê°±ì‹  ì‘ì—… íŠ¸ë¦¬ê±° (ê´€ë¦¬ììš©)
    
    ì°¸ê³ : ì´ ì—”ë“œí¬ì¸íŠ¸ëŠ” Worker í”„ë¡œì„¸ìŠ¤ì™€ ë³„ê°œë¡œ ë™ì‘í•©ë‹ˆë‹¤.
    ì‹¤ì œ ê°±ì‹ ì€ Workerê°€ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # ê°„ë‹¨í•œ ë³´ì•ˆ ì²´í¬ (í”„ë¡œë•ì…˜ì—ì„œëŠ” ë” ê°•ë ¥í•œ ì¸ì¦ í•„ìš”)
    expected_key = os.getenv("ADMIN_API_KEY", "")
    if expected_key and admin_key != expected_key:
        raise HTTPException(status_code=403, detail="Invalid admin key")
    
    try:
        from .workers.ebay_token_worker import run_token_refresh_job
        result = run_token_refresh_job()
        return {
            "success": result.get("success", False),
            "message": "Token refresh job executed",
            "stats": result.get("stats", {}),
            "elapsed_time": result.get("elapsed_time", 0)
        }
    except ImportError:
        raise HTTPException(status_code=500, detail="Worker module not available")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Job failed: {str(e)}")


@app.get("/api/listings")
def get_listings(
    skip: int = 0,
    limit: int = 100,
    store_id: Optional[str] = None,  # Store ID filter - 'all' or None means all stores
    user_id: str = "default-user",  # Default user ID for MVP phase
    db: Session = Depends(get_db)
):
    """Get all listings for a specific user"""
    query = db.query(Listing).filter(Listing.user_id == user_id)
    
    # Apply store filter if store_id is provided and not 'all'
    if store_id and store_id != 'all':
        if hasattr(Listing, 'store_id'):
            query = query.filter(Listing.store_id == store_id)
    # If store_id is 'all' or None, DO NOT filter by store (return all for user)
    
    listings = query.offset(skip).limit(limit).all()
    
    # Get total count with store filter applied
    total_query = db.query(Listing).filter(Listing.user_id == user_id)
    if store_id and store_id != 'all':
        if hasattr(Listing, 'store_id'):
            total_query = total_query.filter(Listing.store_id == store_id)
    total_count = total_query.count()
    
    return {
        "total": total_count,
        "listings": [
            {
                "id": l.id,
                "item_id": getattr(l, 'item_id', None) or getattr(l, 'ebay_item_id', None) or "",
                "ebay_item_id": getattr(l, 'item_id', None) or getattr(l, 'ebay_item_id', None) or "",  # Backward compatibility
                "title": l.title,
                "sku": l.sku,
                "image_url": l.image_url,
                "brand": getattr(l, 'brand', None),
                "upc": getattr(l, 'upc', None),
                "platform": getattr(l, 'platform', None) or getattr(l, 'marketplace', None) or "eBay",
                "marketplace": getattr(l, 'platform', None) or getattr(l, 'marketplace', None) or "eBay",  # Backward compatibility
                "supplier_name": getattr(l, 'supplier_name', None) or (l.metrics.get('supplier_name') if l.metrics and isinstance(l.metrics, dict) else None) or "Unknown",
                "supplier": getattr(l, 'supplier_name', None) or (l.metrics.get('supplier_name') if l.metrics and isinstance(l.metrics, dict) else None) or "Unknown",  # Backward compatibility
                "supplier_id": getattr(l, 'supplier_id', None) or (l.metrics.get('supplier_id') if l.metrics and isinstance(l.metrics, dict) else None),
                "price": (l.metrics.get('price') if l.metrics and isinstance(l.metrics, dict) and 'price' in l.metrics else None) or getattr(l, 'price', None),
                "date_listed": (
                    l.date_listed.isoformat() if l.date_listed else (
                        l.metrics.get('date_listed') if l.metrics and isinstance(l.metrics, dict) and 'date_listed' in l.metrics else None
                    )
                ),
                "sold_qty": (l.metrics.get('sales') if l.metrics and isinstance(l.metrics, dict) and 'sales' in l.metrics else None) or getattr(l, 'sold_qty', 0) or 0,
                "watch_count": (l.metrics.get('views') if l.metrics and isinstance(l.metrics, dict) and 'views' in l.metrics else None) or getattr(l, 'watch_count', 0) or 0,
                # Management hub information (for Shopify detection)
                "management_hub": (
                    l.metrics.get('management_hub') if l.metrics and isinstance(l.metrics, dict) and 'management_hub' in l.metrics else None
                ) or (
                    l.analysis_meta.get('management_hub') if l.analysis_meta and isinstance(l.analysis_meta, dict) and 'management_hub' in l.analysis_meta else None
                ) or getattr(l, 'management_hub', None),
                "raw_data": l.metrics if l.metrics else {},
                "analysis_meta": l.analysis_meta if l.analysis_meta else {}
            }
            for l in listings
        ]
    }


@app.post("/api/listings/detect-source")
def detect_listing_source(
    image_url: str = "",
    sku: str = "",
    title: str = "",
    brand: str = "",
    upc: str = "",
    db: Session = Depends(get_db)
):
    """Detect source for a listing with forensic analysis"""
    source, confidence = detect_source(
        image_url=image_url,
        sku=sku,
        title=title,
        brand=brand,
        upc=upc
    )
    return {
        "source": source,
        "confidence_level": confidence
    }


@app.get("/api/analyze")
def analyze_zombies(
    # ìƒˆ í•„í„° íŒŒë¼ë¯¸í„° (ìˆœì„œëŒ€ë¡œ)
    analytics_period_days: int = 7,  # 1. ë¶„ì„ ê¸°ì¤€ ê¸°ê°„
    min_days: int = 7,               # Legacy compatibility
    max_sales: int = 0,              # 2. ê¸°ê°„ ë‚´ íŒë§¤ ê±´ìˆ˜
    max_watches: int = 0,            # 3. ì°œí•˜ê¸° (Watch)
    max_watch_count: int = 0,        # Legacy compatibility
    max_impressions: int = 100,      # 4. ì´ ë…¸ì¶œ íšŸìˆ˜
    max_views: int = 10,             # 5. ì´ ì¡°íšŒ íšŸìˆ˜
    supplier_filter: str = "All",
    marketplace: str = "eBay",       # MVP Scope: Default to eBay
    store_id: Optional[str] = None,
    user_id: str = "default-user",
    skip: int = 0,
    limit: int = 100,
    db: Session = Depends(get_db)
):
    """
    OptListing ìµœì¢… ì¢€ë¹„ ë¶„ì„ í•„í„° API
    
    í•„í„° ìˆœì„œ (íŒë§¤ â†’ ê´€ì‹¬ â†’ íŠ¸ë˜í”½):
    1. analytics_period_days: ë¶„ì„ ê¸°ì¤€ ê¸°ê°„ (ê¸°ë³¸ 7ì¼)
    2. max_sales: ê¸°ê°„ ë‚´ íŒë§¤ ê±´ìˆ˜ (ê¸°ë³¸ 0ê±´)
    3. max_watches: ì°œí•˜ê¸°/Watch (ê¸°ë³¸ 0ê±´)
    4. max_impressions: ì´ ë…¸ì¶œ íšŸìˆ˜ (ê¸°ë³¸ 100íšŒ ë¯¸ë§Œ)
    5. max_views: ì´ ì¡°íšŒ íšŸìˆ˜ (ê¸°ë³¸ 10íšŒ ë¯¸ë§Œ)
    
    Returns:
    - total_count: Total number of ALL listings in the database
    - total_breakdown: Breakdown by source for ALL listings
    - zombie_count: Number of filtered zombie listings
    - zombies: List of zombie listings (paginated)
    """
    # Validate marketplace - MVP Scope: Only eBay and Shopify
    valid_marketplaces = [
        "eBay",
        "Shopify"
    ]
    if marketplace not in valid_marketplaces:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid marketplace. Must be one of: {', '.join(valid_marketplaces)}"
        )
    
    # Validate supplier_filter
    valid_suppliers = ["All", "Amazon", "Walmart", "Wholesale2B", "Doba", "DSers", "Spocket", "CJ Dropshipping", "Unverified"]
    if supplier_filter not in valid_suppliers:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid supplier_filter. Must be one of: {', '.join(valid_suppliers)}"
        )
    
    # Ensure min_days, max_sales, and max_watch_count are non-negative
    min_days = max(0, min_days)
    max_sales = max(0, max_sales)
    max_watch_count = max(0, max_watch_count)
    
    # Validate and clamp pagination parameters
    skip = max(0, skip)
    limit = min(max(1, limit), 1000)  # Clamp between 1 and 1000
    
    # Check cache for KPI metrics (total_count, total_breakdown, platform_breakdown)
    # Only cache when skip=0 and limit >= 100 (full page requests)
    # Don't cache paginated requests (skip > 0 or limit < 100)
    cache_filters = {
        "min_days": min_days,
        "max_sales": max_sales,
        "max_watch_count": max_watch_count,
        "supplier_filter": supplier_filter
    }
    cache_key = get_cache_key(user_id, store_id, marketplace, cache_filters)
    cached_kpi = None
    
    # Only use cache for full page requests (not paginated)
    if skip == 0 and limit >= 100:
        cached_kpi = get_cached_kpi(cache_key)
    
    # Build base query with user_id filter
    base_query = db.query(Listing).filter(Listing.user_id == user_id)
    
    # Apply store filter if store_id is provided and not 'all'
    if store_id and store_id != 'all':
        if hasattr(Listing, 'store_id'):
            base_query = base_query.filter(Listing.store_id == store_id)
    # If store_id is 'all' or None, DO NOT filter by store (return all for user)
    
    # Get total count using SQL COUNT
    total_count = base_query.count()
    
    # ğŸ”¥ í¬ë ˆë”§ ì°¨ê°: í•„í„°ë§(ë¶„ì„) ìš”ì²­ ì‹œ ì „ì²´ ìŠ¤ìº”í•˜ëŠ” ì œí’ˆ ìˆ˜ë§Œí¼ í¬ë ˆë”§ ì°¨ê°
    # í”„ë¦¬ êµ¬ë… ì‚¬ìš©ìëŠ” ì „ì²´ ë¦¬ìŠ¤íŒ… ìˆ˜ë§Œí¼ í¬ë ˆë”§ì´ ì°¨ê°ë©ë‹ˆë‹¤ (1 ë¦¬ìŠ¤íŒ… = 1 í¬ë ˆë”§)
    # Pro ì´ìƒ êµ¬ë…ìëŠ” í¬ë ˆë”§ ì°¨ê° ì—†ìŒ
    try:
        from .credit_service import deduct_credits_atomic, get_credit_summary
        from fastapi import status as http_status
        import logging
        logger = logging.getLogger(__name__)
        
        # ì‚¬ìš©ì í”„ë¡œí•„ í™•ì¸
        profile = db.query(Profile).filter(Profile.user_id == user_id).first()
        
        # í”„ë¦¬ êµ¬ë…ì¸ ê²½ìš°ì—ë§Œ í¬ë ˆë”§ ì°¨ê°
        if profile and (not profile.subscription_plan or profile.subscription_plan == 'free'):
            # ì „ì²´ ìŠ¤ìº”í•˜ëŠ” ì œí’ˆ ìˆ˜ë§Œí¼ í¬ë ˆë”§ ì°¨ê°
            required_credits = max(1, total_count)  # ìµœì†Œ 1 í¬ë ˆë”§ ì°¨ê°
            
            logger.info(f"ğŸ’° í¬ë ˆë”§ ì°¨ê°: ì „ì²´ {total_count}ê°œ ë¦¬ìŠ¤íŒ… ìŠ¤ìº” â†’ {required_credits} í¬ë ˆë”§ ì°¨ê°")
            
            # í¬ë ˆë”§ ì°¨ê° ì‹œë„
            credit_result = deduct_credits_atomic(
                db=db,
                user_id=user_id,
                amount=required_credits,
                description=f"Zombie listing analysis: {total_count} total listings scanned",
                reference_id=f"analyze_{user_id}_{cache_key}"
            )
            
            if not credit_result.success:
                # í¬ë ˆë”§ ë¶€ì¡±
                raise HTTPException(
                    status_code=http_status.HTTP_402_PAYMENT_REQUIRED,
                    detail={
                        "error": "insufficient_credits",
                        "message": f"í¬ë ˆë”§ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. {required_credits} í¬ë ˆë”§ì´ í•„ìš”í•˜ë©°, í˜„ì¬ {credit_result.remaining_credits} í¬ë ˆë”§ë§Œ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                        "available_credits": credit_result.remaining_credits,
                        "required_credits": required_credits,
                        "listing_count": total_count
                    }
                )
            else:
                logger.info(f"âœ… í¬ë ˆë”§ ì°¨ê° ì™„ë£Œ: {required_credits} í¬ë ˆë”§ ì°¨ê°, ì”ì•¡: {credit_result.remaining_credits}")
        else:
            logger.info(f"âœ… Pro ì´ìƒ êµ¬ë…ì - í¬ë ˆë”§ ì°¨ê° ì—†ìŒ ({total_count}ê°œ ë¦¬ìŠ¤íŒ… ìŠ¤ìº”)")
        # Pro ì´ìƒ êµ¬ë…ìëŠ” í¬ë ˆë”§ ì°¨ê° ì—†ìŒ
    except HTTPException:
        raise  # í¬ë ˆë”§ ë¶€ì¡± ì—ëŸ¬ëŠ” ê·¸ëŒ€ë¡œ ì „ë‹¬
    except Exception as credit_err:
        # í¬ë ˆë”§ ì‹œìŠ¤í…œ ì˜¤ë¥˜ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰ (í¬ë ˆë”§ ì‹œìŠ¤í…œì´ ì—†ì–´ë„ ë¶„ì„ì€ ê°€ëŠ¥í•˜ë„ë¡)
        import logging
        logger = logging.getLogger(__name__)
        logger.warning(f"Credit deduction failed (continuing anyway): {credit_err}")
    
    # Calculate breakdown by supplier using SQL GROUP BY
    supplier_query = db.query(
        Listing.supplier_name,
        func.count(Listing.id).label('count')
    ).filter(
        Listing.user_id == user_id
    )
    
    # Apply store filter to supplier breakdown
    if store_id and store_id != 'all':
        if hasattr(Listing, 'store_id'):
            supplier_query = supplier_query.filter(Listing.store_id == store_id)
    
    supplier_results = supplier_query.group_by(Listing.supplier_name).all()
    
    total_breakdown = {"Amazon": 0, "Walmart": 0, "AliExpress": 0, "CJ Dropshipping": 0, "Home Depot": 0, "Wayfair": 0, "Costco": 0, "Wholesale2B": 0, "Spocket": 0, "SaleHoo": 0, "Inventory Source": 0, "Dropified": 0, "Unverified": 0, "Unknown": 0}
    for supplier_name, count in supplier_results:
        if supplier_name in total_breakdown:
            total_breakdown[supplier_name] = count
        else:
            total_breakdown["Unknown"] = total_breakdown.get("Unknown", 0) + count
    
    # Calculate breakdown by platform using SQL GROUP BY (dynamic - includes all marketplaces)
    # âœ… FIX: platform í•„ë“œê°€ ì—†ìœ¼ë©´ marketplace ì‚¬ìš©
    platform_field = Listing.platform if hasattr(Listing, 'platform') else Listing.marketplace
    platform_query = db.query(
        platform_field,
        func.count(Listing.id).label('count')
    ).filter(
        Listing.user_id == user_id
    )
    
    # Apply store filter to platform breakdown
    if store_id and store_id != 'all':
        if hasattr(Listing, 'store_id'):
            platform_query = platform_query.filter(Listing.store_id == store_id)
    
    platform_results = platform_query.group_by(platform_field).all()
    
    # Build platform breakdown dictionary from SQL results
    platform_breakdown = {}
    for platform, count in platform_results:
        if platform:  # Only include non-null platforms
            platform_breakdown[platform] = count
    
    # Use cached KPI if available, otherwise use freshly calculated values above
    if cached_kpi:
        total_count = cached_kpi.get("total_count", total_count)
        total_breakdown = cached_kpi.get("total_breakdown", total_breakdown)
        platform_breakdown = cached_kpi.get("platform_breakdown", platform_breakdown)
    # If not cached, use the freshly calculated values (total_count, total_breakdown, platform_breakdown)
    # Cache will be set after zombie analysis if this is a full page request
    
    # Get zombie listings (filtered) - pass user_id, skip, and limit
    # Use analytics_period_days if provided, otherwise fall back to min_days
    effective_period = analytics_period_days if analytics_period_days != 7 else min_days
    # Use max_watches if provided, otherwise fall back to max_watch_count
    effective_watches = max_watches if max_watches > 0 else max_watch_count
    
    # ğŸ” ë””ë²„ê¹…: í•„í„° íŒŒë¼ë¯¸í„° ë¡œê¹…
    import logging
    logger = logging.getLogger(__name__)
    logger.info(f"ğŸ” Zombie ë¶„ì„ í•„í„° íŒŒë¼ë¯¸í„°: min_days={effective_period}, max_sales={max_sales}, max_watches={effective_watches}, max_impressions={max_impressions}, max_views={max_views}, supplier_filter={supplier_filter}, platform_filter={marketplace}")
    
    zombies, zombie_breakdown = analyze_zombie_listings(
        db,
        user_id=user_id,
        min_days=effective_period,
        max_sales=max_sales,
        max_watch_count=effective_watches,  # Legacy param
        max_watches=effective_watches,       # New param
        max_impressions=max_impressions,
        max_views=max_views,
        supplier_filter=supplier_filter,
        platform_filter=marketplace,
        store_id=store_id,
        skip=skip,
        limit=limit
    )
    
    logger.info(f"âœ… Zombie ë¶„ì„ ê²°ê³¼: {len(zombies)}ê°œ ì¢€ë¹„ ë°œê²¬ (ì „ì²´ {total_count}ê°œ ì¤‘)")
    
    # Cache KPI metrics if this is a full page request
    if skip == 0 and limit >= 100 and not cached_kpi:
        kpi_data = {
            "total_count": total_count,
            "total_breakdown": total_breakdown,
            "platform_breakdown": platform_breakdown
        }
        set_cached_kpi(cache_key, kpi_data)
    
    return {
        "total_count": total_count,
        "total_breakdown": total_breakdown,
        "platform_breakdown": platform_breakdown,
        "zombie_count": len(zombies),
        "zombie_breakdown": zombie_breakdown,  # Store-Level Breakdown
        "zombies": [
            {
                "id": z.id,
                "item_id": getattr(z, 'item_id', None) or getattr(z, 'ebay_item_id', None) or "",
                "ebay_item_id": getattr(z, 'item_id', None) or getattr(z, 'ebay_item_id', None) or "",  # Backward compatibility
                "title": z.title,
                "sku": z.sku,
                "image_url": z.image_url,
                "platform": getattr(z, 'platform', None) or getattr(z, 'marketplace', None) or "eBay",
                "marketplace": getattr(z, 'platform', None) or getattr(z, 'marketplace', None) or "eBay",  # Backward compatibility
                "supplier_name": getattr(z, 'supplier_name', None) or "Unknown",
                "supplier": getattr(z, 'supplier_name', None) or "Unknown",  # Backward compatibility
                "supplier_id": getattr(z, 'supplier_id', None),
                "price": (z.metrics.get('price') if z.metrics and 'price' in z.metrics else None) or z.price,
                "date_listed": z.date_listed.isoformat() if z.date_listed else None,
                "sold_qty": (z.metrics.get('sales') if z.metrics and isinstance(z.metrics, dict) and 'sales' in z.metrics else None) or z.sold_qty or 0,
                "quantity_sold": (z.metrics.get('sales') if z.metrics and isinstance(z.metrics, dict) and 'sales' in z.metrics else None) or z.sold_qty or 0,
                "total_sales": (z.metrics.get('sales') if z.metrics and isinstance(z.metrics, dict) and 'sales' in z.metrics else None) or z.sold_qty or 0,
                "watch_count": (z.metrics.get('watches') if z.metrics and isinstance(z.metrics, dict) and 'watches' in z.metrics else None) or z.watch_count or 0,
                "view_count": (z.metrics.get('views') if z.metrics and isinstance(z.metrics, dict) and 'views' in z.metrics else None) or getattr(z, 'view_count', None) or 0,
                "views": (z.metrics.get('views') if z.metrics and isinstance(z.metrics, dict) and 'views' in z.metrics else None) or getattr(z, 'view_count', None) or 0,
                "impressions": (z.metrics.get('impressions') if z.metrics and isinstance(z.metrics, dict) and 'impressions' in z.metrics else None) or getattr(z, 'impressions', None) or 0,
                "is_global_winner": bool(getattr(z, 'is_global_winner', 0)),  # Cross-Platform Health Check flag
                "is_active_elsewhere": bool(getattr(z, 'is_active_elsewhere', 0)),  # Cross-Platform Activity Check flag
                # Management hub information (for Shopify detection)
                "management_hub": (
                    z.metrics.get('management_hub') if z.metrics and isinstance(z.metrics, dict) and 'management_hub' in z.metrics else None
                ) or (
                    z.analysis_meta.get('management_hub') if z.analysis_meta and isinstance(z.analysis_meta, dict) and 'management_hub' in z.analysis_meta else None
                ) or getattr(z, 'management_hub', None),
                "raw_data": z.metrics if z.metrics else {},
                "analysis_meta": z.analysis_meta if z.analysis_meta else {}
            }
            for z in zombies
        ]
    }


@app.post("/api/export")
def export_csv(
    export_mode: str,  # "autods", "yaballe", "ebay"
    min_days: int = 60,
    max_sales: int = 0,
    max_watch_count: int = 10,
    supplier_filter: str = "All",
    db: Session = Depends(get_db)
):
    """
    Smart Export Feature
    Generates CSV file based on the selected Listing Tool.
    Uses the same filter parameters as /api/analyze
    """
    if export_mode not in ["autods", "yaballe", "ebay"]:
        raise HTTPException(
            status_code=400,
            detail="Invalid export_mode. Must be one of: autods, yaballe, ebay"
        )
    
    # Validate supplier_filter
    valid_suppliers = ["All", "Amazon", "Walmart", "AliExpress", "CJ Dropshipping", "Home Depot", "Wayfair", "Costco", "Wholesale2B", "Spocket", "SaleHoo", "Inventory Source", "Dropified", "Unverified", "Unknown"]
    if supplier_filter not in valid_suppliers:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid supplier_filter. Must be one of: {', '.join(valid_suppliers)}"
        )
    
    # Ensure min_days, max_sales, and max_watch_count are non-negative
    min_days = max(0, min_days)
    max_sales = max(0, max_sales)
    max_watch_count = max(0, max_watch_count)
    
    # Get zombie listings with filters
    zombies, _ = analyze_zombie_listings(
        db, 
        user_id="default-user",  # Default user ID
        min_days=min_days, 
        max_sales=max_sales,
        max_watch_count=max_watch_count,
        supplier_filter=supplier_filter
    )
    
    if not zombies:
        raise HTTPException(status_code=404, detail="No zombie listings found")
    
    # Generate CSV (with snapshot logging)
    csv_content = generate_export_csv(zombies, export_mode, db=db, user_id="default-user")
    
    # Determine filename
    filename_map = {
        "autods": "zombies_autods.csv",
        "yaballe": "zombies_yaballe.csv",
        "ebay": "zombies_ebay.csv"
    }
    
    return Response(
        content=csv_content,
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename={filename_map[export_mode]}"
        }
    )


class ExportQueueRequest(BaseModel):
    items: List[Dict]
    target_tool: Optional[str] = None  # New parameter: "autods", "wholesale2b", "shopify_matrixify", etc.
    export_mode: Optional[str] = None  # Legacy parameter for backward compatibility
    mode: Optional[str] = "delete_list"  # "delete_list" (default) or "full_sync_list"
    store_id: Optional[str] = None  # Store ID for full_sync_list mode

@app.post("/api/export-queue")
def export_queue_csv(
    request: ExportQueueRequest,
    db: Session = Depends(get_db)
):
    """
    Export CSV from queue items (staging area)
    Accepts a list of items directly from the frontend queue
    Supports multiple export formats via target_tool parameter.
    """
    items = request.items
    
    # Support both new target_tool and legacy export_mode
    target_tool = request.target_tool or request.export_mode or "autods"
    
    valid_tools = ["autods", "yaballe", "ebay", "wholesale2b", "shopify_matrixify", "shopify_tagging"]
    if target_tool not in valid_tools:
        raise HTTPException(
            status_code=400,
            detail=f"Invalid target_tool. Must be one of: {', '.join(valid_tools)}"
        )
    
    if not items:
        raise HTTPException(status_code=400, detail="No items in queue to export")
    
    # Validate mode
    mode = request.mode or "delete_list"
    if mode not in ["delete_list", "full_sync_list"]:
        raise HTTPException(
            status_code=400,
            detail="Invalid mode. Must be 'delete_list' or 'full_sync_list'"
        )
    
    # Generate CSV directly from items (dictionaries) with target_tool (with snapshot logging)
    csv_content = generate_export_csv(
        items, 
        target_tool, 
        db=db, 
        user_id="default-user",
        mode=mode,
        store_id=request.store_id
    )
    
    # Determine filename
    filename_map = {
        "autods": "queue_autods.csv",
        "yaballe": "queue_yaballe.csv",
        "ebay": "queue_ebay.csv",
        "wholesale2b": "queue_wholesale2b.csv",
        "shopify_matrixify": "queue_shopify_matrixify.csv",
        "shopify_tagging": "queue_shopify_tagging.csv"
    }
    
    return Response(
        content=csv_content,
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename={filename_map[target_tool]}"
        }
    )


class LogDeletionRequest(BaseModel):
    items: List[Dict]

@app.post("/api/log-deletion")
def log_deletion(
    request: LogDeletionRequest,
    db: Session = Depends(get_db)
):
    """
    Log deleted items to history
    Accepts a list of items from the queue that were exported/deleted
    Uses supplier_name (not source) and stores full snapshot in JSONB
    """
    items = request.items
    
    if not items:
        raise HTTPException(status_code=400, detail="No items to log")
    
    # Create deletion log entries
    logs = []
    for item in items:
        # Extract supplier_name (prefer supplier_name over supplier over source)
        supplier = item.get("supplier_name") or item.get("supplier") or item.get("source", "Unknown")
        
        # Extract platform/marketplace
        platform = item.get("platform") or item.get("marketplace") or "eBay"
        
        # Create snapshot JSONB with full item data for future reference
        snapshot_data = {
            "supplier_name": supplier,
            "supplier_id": item.get("supplier_id"),
            "platform": platform,
            "title": item.get("title", "Unknown"),
            "price": item.get("price"),
            "sold_qty": item.get("sold_qty"),
            "watch_count": item.get("watch_count"),
            # Include all other fields for completeness
            **{k: v for k, v in item.items() if k not in ["supplier_name", "supplier", "source", "platform", "marketplace"]}
        }
        
        log_entry = DeletionLog(
            item_id=item.get("ebay_item_id") or item.get("item_id") or str(item.get("id", "")),
            title=item.get("title", "Unknown"),
            platform=platform,
            supplier=supplier,  # Use supplier_name (not source)
            snapshot=snapshot_data  # Store full snapshot in JSONB
        )
        logs.append(log_entry)
    
    # Bulk insert
    try:
        db.add_all(logs)
        db.commit()
    except Exception as e:
        db.rollback()
        print(f"Error logging deletions: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Failed to log deletions: {str(e)}")
    
    return {
        "message": f"Logged {len(logs)} deletions",
        "count": len(logs)
    }


@app.get("/api/history")
def get_deletion_history(
    skip: int = 0,
    limit: int = 1000,
    db: Session = Depends(get_db)
):
    """
    Get deletion history
    Returns total count and list of deleted items (most recent first)
    Uses supplier field from DeletionLog (not source) and safely handles JSONB snapshot
    """
    try:
        # Validate pagination parameters
        skip = max(0, skip)
        limit = min(max(1, limit), 10000)  # Clamp between 1 and 10000
        
        # Get total count
        total_count = db.query(DeletionLog).count()
        
        # Get logs (most recent first)
        logs = db.query(DeletionLog).order_by(DeletionLog.deleted_at.desc()).offset(skip).limit(limit).all()
        
        # Build response with safe field access
        log_list = []
        for log in logs:
            # Safely extract supplier (handle NULL and fallback to snapshot or "Unknown")
            supplier = None
            if hasattr(log, 'supplier') and log.supplier:
                supplier = log.supplier
            elif hasattr(log, 'snapshot') and log.snapshot:
                # Try to get supplier from snapshot JSONB if available
                if isinstance(log.snapshot, dict):
                    supplier = log.snapshot.get('supplier_name') or log.snapshot.get('supplier') or log.snapshot.get('source')
                elif isinstance(log.snapshot, str):
                    try:
                        import json
                        snapshot_dict = json.loads(log.snapshot)
                        supplier = snapshot_dict.get('supplier_name') or snapshot_dict.get('supplier') or snapshot_dict.get('source')
                    except:
                        pass
            
            # Default to "Unknown" if supplier is still None
            supplier = supplier or "Unknown"
            
            # Safely extract platform
            platform = None
            if hasattr(log, 'platform') and log.platform:
                platform = log.platform
            elif hasattr(log, 'snapshot') and log.snapshot:
                if isinstance(log.snapshot, dict):
                    platform = log.snapshot.get('platform') or log.snapshot.get('marketplace')
                elif isinstance(log.snapshot, str):
                    try:
                        import json
                        snapshot_dict = json.loads(log.snapshot)
                        platform = snapshot_dict.get('platform') or snapshot_dict.get('marketplace')
                    except:
                        pass
            
            platform = platform or "eBay"  # Default platform
            
            log_list.append({
                "id": log.id,
                "item_id": log.item_id if hasattr(log, 'item_id') else "",
                "title": log.title if hasattr(log, 'title') else "Unknown",
                "platform": platform,
                "supplier": supplier,
                "deleted_at": log.deleted_at.isoformat() if hasattr(log, 'deleted_at') and log.deleted_at else None
            })
        
        return {
            "total_count": total_count,
            "logs": log_list
        }
    except Exception as e:
        # Log error with full traceback
        print(f"Error fetching deletion history: {e}")
        import traceback
        traceback.print_exc()
        # Return error response with CORS headers (not empty response)
        raise HTTPException(
            status_code=500,
            detail=f"Failed to fetch deletion history: {str(e)}"
        )


class UpdateListingRequest(BaseModel):
    supplier: Optional[str] = None
    supplier_name: Optional[str] = None
    supplier_id: Optional[str] = None

@app.patch("/api/listing/{listing_id}")
def update_listing(
    listing_id: int,
    request: UpdateListingRequest,
    db: Session = Depends(get_db)
):
    """
    Update a listing's supplier (manual override)
    Allows users to correct auto-detected suppliers
    Supports both supplier (legacy) and supplier_name/supplier_id (new format)
    """
    listing = db.query(Listing).filter(Listing.id == listing_id).first()
    if not listing:
        raise HTTPException(status_code=404, detail="Listing not found")
    
    # Support legacy 'supplier' field for backward compatibility
    supplier_name = request.supplier_name or request.supplier
    supplier_id = request.supplier_id
    
    # Validate supplier if provided
    if supplier_name is not None:
        valid_suppliers = ["Amazon", "Walmart", "AliExpress", "CJ Dropshipping", "Home Depot", "Wayfair", "Costco", "Wholesale2B", "Spocket", "SaleHoo", "Inventory Source", "Dropified", "Unverified", "Unknown", "AutoDS", "Yaballe"]
        if supplier_name not in valid_suppliers:
            raise HTTPException(
                status_code=400,
                detail=f"Invalid supplier. Must be one of: {', '.join(valid_suppliers)}"
            )
        listing.supplier_name = supplier_name
    
    # Update supplier_id if provided
    if supplier_id is not None:
        listing.supplier_id = supplier_id if supplier_id else None
    
    db.commit()
    db.refresh(listing)
    
    return {
        "id": listing.id,
        "item_id": getattr(listing, 'item_id', None) or getattr(listing, 'ebay_item_id', None) or "",
        "ebay_item_id": getattr(listing, 'item_id', None) or getattr(listing, 'ebay_item_id', None) or "",  # Backward compatibility
        "title": listing.title,
        "supplier_name": listing.supplier_name,
        "supplier_id": listing.supplier_id,
        "supplier": listing.supplier_name,  # Backward compatibility
        "message": "Listing updated successfully"
    }


@app.post("/api/dummy-data")
def create_dummy_data(
    count: int = 50,
    user_id: str = "default-user",
    db: Session = Depends(get_db)
):
    """Generate dummy listings for testing with new hybrid schema"""
    # Ensure tables exist before attempting to delete
    Base.metadata.create_all(bind=engine)
    
    # Clear all existing listings before generating new dummy data
    db.query(Listing).delete()
    db.commit()
    
    generate_dummy_listings(db, count=count, user_id=user_id)
    return {"message": f"Generated {count} dummy listings"}


# ============================================================
# CSV Upload Endpoints - ê³µê¸‰ì²˜ CSV íŒŒì´í”„ë¼ì¸
# ============================================================

from fastapi import File, UploadFile
from .csv_processor import (
    process_supplier_csv,
    generate_csv_template,
    get_unmatched_listings,
    CSVProcessingResult
)


class CSVUploadResponse(BaseModel):
    """CSV ì—…ë¡œë“œ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    success: bool
    message: str
    result: Optional[Dict] = None


@app.post("/api/upload-supplier-csv", response_model=CSVUploadResponse)
async def upload_supplier_csv(
    file: UploadFile = File(...),
    user_id: str = "default-user",
    dry_run: bool = False,
    db: Session = Depends(get_db)
):
    """
    ê³µê¸‰ì²˜ CSV íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬
    
    - **file**: CSV íŒŒì¼ (í•„ìˆ˜)
    - **user_id**: ì‚¬ìš©ì ID
    - **dry_run**: Trueë©´ ì‹¤ì œ DB ì—…ë°ì´íŠ¸ ì—†ì´ ì‹œë®¬ë ˆì´ì…˜
    
    ì§€ì› CSV í˜•ì‹:
    - SKU, UPC, EAN ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ ì´ìƒ í•„ìˆ˜
    - SupplierName ì»¬ëŸ¼ í•„ìˆ˜
    """
    try:
        # íŒŒì¼ íƒ€ì… ê²€ì¦
        if not file.filename.endswith(('.csv', '.CSV')):
            raise HTTPException(
                status_code=400,
                detail="CSV íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤"
            )
        
        # íŒŒì¼ í¬ê¸° ì œí•œ (10MB)
        content = await file.read()
        if len(content) > 10 * 1024 * 1024:
            raise HTTPException(
                status_code=400,
                detail="íŒŒì¼ í¬ê¸°ëŠ” 10MB ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤"
            )
        
        # CSV ì²˜ë¦¬
        result = process_supplier_csv(
            file_content=content,
            user_id=user_id,
            dry_run=dry_run
        )
        
        # ê²°ê³¼ ë°˜í™˜
        return CSVUploadResponse(
            success=result.updated_listings > 0 or result.matched_listings > 0,
            message=f"ì²˜ë¦¬ ì™„ë£Œ: {result.total_rows}ê°œ í–‰ ì¤‘ {result.matched_listings}ê°œ ë§¤ì¹­, {result.updated_listings}ê°œ ì—…ë°ì´íŠ¸",
            result={
                "total_rows": result.total_rows,
                "valid_rows": result.valid_rows,
                "invalid_rows": result.invalid_rows,
                "matched_listings": result.matched_listings,
                "updated_listings": result.updated_listings,
                "unmatched_rows": result.unmatched_rows,
                "processing_time_ms": result.processing_time_ms,
                "match_details": result.match_details,
                "errors": result.errors[:10]  # ìµœëŒ€ 10ê°œ ì—ëŸ¬ë§Œ ë°˜í™˜
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"CSV ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}"
        )


@app.get("/api/csv-template")
def download_csv_template():
    """
    ê³µê¸‰ì²˜ CSV í…œí”Œë¦¿ ë‹¤ìš´ë¡œë“œ
    ì‚¬ìš©ìê°€ ì—…ë¡œë“œí•  CSV í˜•ì‹ ì˜ˆì‹œ
    """
    template = generate_csv_template()
    
    return Response(
        content=template,
        media_type="text/csv",
        headers={
            "Content-Disposition": "attachment; filename=supplier_template.csv"
        }
    )


@app.get("/api/unmatched-listings")
def get_unmatched_listings_endpoint(
    user_id: str = "default-user",
    limit: int = 100,
    db: Session = Depends(get_db)
):
    """
    ê³µê¸‰ì²˜ ì •ë³´ê°€ ì—†ëŠ” ë¦¬ìŠ¤íŒ… ëª©ë¡ ì¡°íšŒ
    ì‚¬ìš©ìê°€ CSVì— ì¶”ê°€í•´ì•¼ í•  ë¦¬ìŠ¤íŒ…ë“¤
    """
    try:
        listings = get_unmatched_listings(db, user_id, limit)
        
        return {
            "count": len(listings),
            "listings": listings,
            "message": f"ê³µê¸‰ì²˜ ì •ë³´ê°€ ì—†ëŠ” ë¦¬ìŠ¤íŒ… {len(listings)}ê°œ"
        }
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=500,
            detail=f"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
        )


# ============================================================
# Credit Management API - 3-Way Hybrid Pricing
# ============================================================

class AnalysisStartRequest(BaseModel):
    """ë¶„ì„ ì‹œì‘ ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    listing_ids: Optional[List[int]] = None  # íŠ¹ì • ë¦¬ìŠ¤íŒ…ë§Œ ë¶„ì„
    listing_count: Optional[int] = None  # ë˜ëŠ” ë¦¬ìŠ¤íŒ… ìˆ˜ ì§ì ‘ ì§€ì •
    analysis_type: str = "zombie"  # "zombie", "full", "quick"
    marketplace: str = "eBay"


class AnalysisStartResponse(BaseModel):
    """ë¶„ì„ ì‹œì‘ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    success: bool
    analysis_id: str
    listings_to_analyze: int
    credits_deducted: int
    remaining_credits: int
    message: str


class CreditBalanceResponse(BaseModel):
    """í¬ë ˆë”§ ì”ì•¡ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    user_id: str
    purchased_credits: int
    consumed_credits: int
    available_credits: int
    current_plan: str


class AddCreditsRequest(BaseModel):
    """í¬ë ˆë”§ ì¶”ê°€ ìš”ì²­ ìŠ¤í‚¤ë§ˆ (ê²°ì œ í›„ ì—°ë™)"""
    amount: int
    transaction_type: str = "purchase"  # "purchase", "bonus", "refund"
    reference_id: Optional[str] = None  # ê²°ì œ ID ë“±
    description: Optional[str] = None


@app.get("/api/credits", response_model=CreditBalanceResponse)
def get_credit_balance(
    user_id: str = "default-user",
    db: Session = Depends(get_db)
):
    """
    ì‚¬ìš©ì í¬ë ˆë”§ ì”ì•¡ ì¡°íšŒ
    
    Returns:
    - purchased_credits: ì´ êµ¬ë§¤/ë¶€ì—¬ëœ í¬ë ˆë”§
    - consumed_credits: ì´ ì‚¬ìš©ëœ í¬ë ˆë”§
    - available_credits: ì‚¬ìš© ê°€ëŠ¥í•œ í¬ë ˆë”§ (purchased - consumed)
    - current_plan: í˜„ì¬ í”Œëœ (free, starter, pro, enterprise)
    """
    summary = get_credit_summary(db, user_id)
    
    # í”„ë¡œí•„ì´ ì—†ìœ¼ë©´ ìë™ ìƒì„±
    if not summary.get("exists"):
        result = initialize_user_credits(db, user_id, PlanType.FREE)
        summary = get_credit_summary(db, user_id)
    
    return CreditBalanceResponse(
        user_id=user_id,
        purchased_credits=summary["purchased_credits"],
        consumed_credits=summary["consumed_credits"],
        available_credits=summary["available_credits"],
        current_plan=summary["current_plan"]
    )


@app.post("/api/analysis/start", response_model=AnalysisStartResponse)
def start_analysis(
    request: AnalysisStartRequest,
    user_id: str = "default-user",
    db: Session = Depends(get_db)
):
    """
    ë¦¬ìŠ¤íŒ… ë¶„ì„ ì‹œì‘ (í¬ë ˆë”§ ê²€ì‚¬ ë° ì°¨ê°)
    
    **í¬ë ˆë”§ ì •ì±…:**
    - 1 ë¦¬ìŠ¤íŒ… = 1 í¬ë ˆë”§ (ê¸°ë³¸)
    - ì”ì•¡ ë¶€ì¡± ì‹œ 402 Payment Required ë°˜í™˜
    
    **ìš”ì²­:**
    - listing_ids: ë¶„ì„í•  ë¦¬ìŠ¤íŒ… ID ëª©ë¡ (ì„ íƒ)
    - listing_count: ë˜ëŠ” ë¦¬ìŠ¤íŒ… ìˆ˜ ì§ì ‘ ì§€ì • (ì„ íƒ)
    - analysis_type: "zombie" (ê¸°ë³¸), "full", "quick"
    
    **ì‘ë‹µ:**
    - analysis_id: ë¶„ì„ ì‘ì—… ID (ì¶”ì ìš©)
    - credits_deducted: ì°¨ê°ëœ í¬ë ˆë”§
    - remaining_credits: ë‚¨ì€ í¬ë ˆë”§
    """
    import uuid
    
    # 1. ë¶„ì„í•  ë¦¬ìŠ¤íŒ… ìˆ˜ ê²°ì •
    if request.listing_ids:
        listing_count = len(request.listing_ids)
    elif request.listing_count:
        listing_count = request.listing_count
    else:
        # listing_idsì™€ listing_count ëª¨ë‘ ì—†ìœ¼ë©´ ì „ì²´ ë¦¬ìŠ¤íŒ… ìˆ˜ ì¡°íšŒ
        listing_count = db.query(Listing).filter(
            Listing.user_id == user_id
        ).count()
    
    if listing_count <= 0:
        raise HTTPException(
            status_code=400,
            detail="ë¶„ì„í•  ë¦¬ìŠ¤íŒ…ì´ ì—†ìŠµë‹ˆë‹¤"
        )
    
    # 2. í¬ë ˆë”§ ê²€ì‚¬ ë° ì›ìì  ì°¨ê°
    analysis_id = str(uuid.uuid4())
    
    result = deduct_credits_atomic(
        db=db,
        user_id=user_id,
        amount=listing_count,
        description=f"Analysis ({request.analysis_type}): {listing_count} listings",
        reference_id=analysis_id
    )
    
    # 3. í¬ë ˆë”§ ë¶€ì¡± ì‹œ 402 ë°˜í™˜
    if not result.success:
        raise HTTPException(
            status_code=402,  # Payment Required
            detail={
                "error": "insufficient_credits",
                "message": result.message,
                "available_credits": result.remaining_credits,
                "required_credits": listing_count,
                "purchase_url": "/pricing"  # ê²°ì œ í˜ì´ì§€ URL
            }
        )
    
    # 4. ë¶„ì„ ì‘ì—… ì‹œì‘ (ì‹¤ì œ ë¶„ì„ ë¡œì§ ì—°ë™)
    # analyze_zombie_listings í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰
    try:
        zombies, zombie_breakdown = analyze_zombie_listings(
            db=db,
            user_id=user_id,
            min_days=request.analytics_period_days or request.min_days or 7,
            max_sales=request.max_sales or 0,
            max_watches=request.max_watches or request.max_watch_count or 0,
            max_impressions=request.max_impressions or 100,
            max_views=request.max_views or 10,
            supplier_filter=request.supplier_filter or "All",
            platform_filter=request.marketplace or "eBay",
            store_id=request.store_id,
            skip=0,  # ë¶„ì„ ì‘ì—…ì€ ì „ì²´ ë¦¬ìŠ¤íŒ… ëŒ€ìƒ
            limit=10000  # ì¶©ë¶„íˆ í° ê°’ìœ¼ë¡œ ì„¤ì •
        )
        
        zombie_count = len(zombies)
        logger.info(f"âœ… Analysis completed: {zombie_count} zombie listings found out of {listing_count} total listings")
        
        return AnalysisStartResponse(
            success=True,
            analysis_id=analysis_id,
            listings_to_analyze=listing_count,
            credits_deducted=result.deducted_amount,
            remaining_credits=result.remaining_credits,
            message=f"ë¶„ì„ ì™„ë£Œ: {listing_count}ê°œ ë¦¬ìŠ¤íŒ… ì¤‘ {zombie_count}ê°œ Low-Performing ë°œê²¬, {result.deducted_amount} í¬ë ˆë”§ ì°¨ê°"
        )
    except Exception as e:
        logger.error(f"âŒ Analysis failed: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        # ë¶„ì„ ì‹¤íŒ¨ ì‹œì—ë„ í¬ë ˆë”§ì€ ì´ë¯¸ ì°¨ê°ë˜ì—ˆìœ¼ë¯€ë¡œ ì„±ê³µ ì‘ë‹µ ë°˜í™˜
        # (í¬ë ˆë”§ í™˜ë¶ˆì€ ë³„ë„ í”„ë¡œì„¸ìŠ¤ë¡œ ì²˜ë¦¬)
        return AnalysisStartResponse(
            success=True,
            analysis_id=analysis_id,
            listings_to_analyze=listing_count,
            credits_deducted=result.deducted_amount,
            remaining_credits=result.remaining_credits,
            message=f"ë¶„ì„ ì‹œì‘: {listing_count}ê°œ ë¦¬ìŠ¤íŒ…, {result.deducted_amount} í¬ë ˆë”§ ì°¨ê° (ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)})"
        )


@app.post("/api/credits/add")
def add_user_credits(
    request: AddCreditsRequest,
    user_id: str = "default-user",
    admin_key: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """
    í¬ë ˆë”§ ì¶”ê°€ (ê²°ì œ ì™„ë£Œ í›„ ë˜ëŠ” ê´€ë¦¬ììš©)
    
    **ë³´ì•ˆ:** í”„ë¡œë•ì…˜ì—ì„œëŠ” Webhook ë˜ëŠ” Admin API Key ê²€ì¦ í•„ìš”
    
    **ìš”ì²­:**
    - amount: ì¶”ê°€í•  í¬ë ˆë”§ ìˆ˜
    - transaction_type: "purchase", "bonus", "refund"
    - reference_id: ê²°ì œ ID (ì„ íƒ)
    """
    # ê°„ë‹¨í•œ ë³´ì•ˆ ì²´í¬ (í”„ë¡œë•ì…˜ì—ì„œëŠ” Webhook ì‹œê·¸ë‹ˆì²˜ ê²€ì¦ìœ¼ë¡œ ëŒ€ì²´)
    expected_key = os.getenv("ADMIN_API_KEY", "")
    if expected_key and admin_key != expected_key:
        # Admin keyê°€ ì„¤ì •ë˜ì–´ ìˆê³  ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ê±°ë¶€
        # ë‹¨, Lemon Squeezy Webhookì—ì„œëŠ” ë³„ë„ ê²€ì¦
        pass  # MVPì—ì„œëŠ” í—ˆìš© (TODO: í”„ë¡œë•ì…˜ì—ì„œ ê°•í™”)
    
    if request.amount <= 0:
        raise HTTPException(status_code=400, detail="Amount must be positive")
    
    # TransactionType ë³€í™˜
    try:
        tx_type = TransactionType(request.transaction_type)
    except ValueError:
        tx_type = TransactionType.PURCHASE
    
    result = add_credits(
        db=db,
        user_id=user_id,
        amount=request.amount,
        transaction_type=tx_type,
        description=request.description,
        reference_id=request.reference_id
    )
    
    if not result.success:
        raise HTTPException(status_code=400, detail=result.message)
    
    return {
        "success": True,
        "added_credits": result.added_amount,
        "total_credits": result.total_credits,
        "transaction_id": result.transaction_id,
        "message": result.message
    }


@app.post("/api/credits/refund")
def refund_user_credits(
    amount: int,
    reason: str,
    user_id: str = "default-user",
    reference_id: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """
    í¬ë ˆë”§ í™˜ë¶ˆ (ë¶„ì„ ì‹¤íŒ¨ ì‹œ ë“±)
    
    **ì‚¬ìš© ì‚¬ë¡€:**
    - ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ í™˜ë¶ˆ
    - ê³ ê° ì„œë¹„ìŠ¤ í™˜ë¶ˆ
    """
    if amount <= 0:
        raise HTTPException(status_code=400, detail="Amount must be positive")
    
    result = refund_credits(
        db=db,
        user_id=user_id,
        amount=amount,
        reason=reason,
        reference_id=reference_id
    )
    
    if not result.success:
        raise HTTPException(status_code=400, detail=result.message)
    
    return {
        "success": True,
        "refunded_credits": result.added_amount,
        "total_credits": result.total_credits,
        "message": f"í™˜ë¶ˆ ì™„ë£Œ: {amount} í¬ë ˆë”§ ({reason})"
    }


@app.post("/api/credits/initialize")
def initialize_credits(
    user_id: str,
    plan: str = "free",
    db: Session = Depends(get_db)
):
    """
    ì‹ ê·œ ì‚¬ìš©ì í¬ë ˆë”§ ì´ˆê¸°í™”
    
    **ì‚¬ìš© ì‚¬ë¡€:**
    - íšŒì› ê°€ì… ì‹œ ìë™ í˜¸ì¶œ
    - í”Œëœ ì—…ê·¸ë ˆì´ë“œ ì‹œ ë³´ë„ˆìŠ¤ í¬ë ˆë”§ ì§€ê¸‰
    """
    try:
        plan_type = PlanType(plan)
    except ValueError:
        plan_type = PlanType.FREE
    
    result = initialize_user_credits(db, user_id, plan_type)
    
    return {
        "success": result.success,
        "total_credits": result.total_credits,
        "message": result.message
    }


# ============================================================
# Lemon Squeezy Webhook
# ============================================================

@app.post("/webhooks/lemonsqueezy")
async def lemonsqueezy_webhook(request: Request, db: Session = Depends(get_db)):
    """
    Lemon Squeezy ì›¹í›… ì—”ë“œí¬ì¸íŠ¸
    ì•ˆì •ì„± ì›ì¹™: ëª¨ë“  ì—ëŸ¬ëŠ” ë¡œê¹…í•˜ê³  200 OK ë°˜í™˜ (LS ì¬ì‹œë„ ë°©ì§€)
    """
    import logging
    logger = logging.getLogger(__name__)
    
    try:
        # ì›ë³¸ ìš”ì²­ ë³¸ë¬¸ ì½ê¸° (ì‹œê·¸ë‹ˆì²˜ ê²€ì¦ìš©)
        body = await request.body()
        
        # X-Signature í—¤ë” í™•ì¸
        signature = request.headers.get("X-Signature", "")
        
        # ì‹œê·¸ë‹ˆì²˜ ê²€ì¦
        if not verify_webhook_signature(body, signature):
            logger.error("ì›¹í›… ì‹œê·¸ë‹ˆì²˜ ê²€ì¦ ì‹¤íŒ¨")
            # ì•ˆì •ì„±: ê²€ì¦ ì‹¤íŒ¨ ì‹œì—ë„ 200 OK ë°˜í™˜ (LS ì¬ì‹œë„ ë°©ì§€)
            # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” 401ì„ ë°˜í™˜í•  ìˆ˜ë„ ìˆì§€ë§Œ, ë¡œê¹…ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§
            return JSONResponse(
                status_code=200,  # LS ì¬ì‹œë„ ë°©ì§€ë¥¼ ìœ„í•´ 200 ë°˜í™˜
                content={"status": "error", "message": "Invalid signature"},
                headers={"X-Webhook-Status": "invalid_signature"}
            )
        
        # JSON íŒŒì‹±
        try:
            event_data = json.loads(body.decode('utf-8'))
        except json.JSONDecodeError as e:
            logger.error(f"ì›¹í›… JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return JSONResponse(
                status_code=200,
                content={"status": "error", "message": "Invalid JSON"}
            )
        
        # ì´ë²¤íŠ¸ ì²˜ë¦¬
        try:
            success = process_webhook_event(db, event_data)
            
            if success:
                logger.info("ì›¹í›… ì´ë²¤íŠ¸ ì²˜ë¦¬ ì„±ê³µ")
                return JSONResponse(
                    status_code=200,
                    content={"status": "success", "message": "Webhook processed"}
                )
            else:
                logger.warning("ì›¹í›… ì´ë²¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ (ë¡œê¹…ë¨)")
                # ì•ˆì •ì„±: ì²˜ë¦¬ ì‹¤íŒ¨í•´ë„ 200 OK ë°˜í™˜ (ì—ëŸ¬ëŠ” ë¡œê¹…ë¨)
                return JSONResponse(
                    status_code=200,
                    content={"status": "error", "message": "Processing failed (logged)"}
                )
                
        except Exception as e:
            logger.error(f"ì›¹í›… ì´ë²¤íŠ¸ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            # ì•ˆì •ì„±: ì˜ˆì™¸ ë°œìƒí•´ë„ 200 OK ë°˜í™˜
            return JSONResponse(
                status_code=200,
                content={"status": "error", "message": "Internal error (logged)"}
            )
            
    except Exception as e:
        logger.error(f"ì›¹í›… ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
        # ì•ˆì •ì„±: ëª¨ë“  ì˜ˆì™¸ë¥¼ ì¡ì•„ì„œ 200 OK ë°˜í™˜
        return JSONResponse(
            status_code=200,
            content={"status": "error", "message": "Request processing failed (logged)"}
        )


# ============================================================
# CSV Format Management API
# ============================================================

@app.get("/api/csv-formats")
def get_csv_formats(
    supplier_name: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """
    CSV í¬ë§· ëª©ë¡ ì¡°íšŒ
    supplier_nameì´ ì œê³µë˜ë©´ í•´ë‹¹ í¬ë§·ë§Œ ë°˜í™˜
    """
    if supplier_name:
        csv_format = db.query(CSVFormat).filter(
            CSVFormat.supplier_name == supplier_name,
            CSVFormat.is_active == True
        ).first()
        if not csv_format:
            raise HTTPException(status_code=404, detail=f"CSV format not found for supplier: {supplier_name}")
        return {
            "supplier_name": csv_format.supplier_name,
            "display_name": csv_format.display_name,
            "description": csv_format.description,
            "format_schema": csv_format.format_schema,
            "is_active": csv_format.is_active
        }
    else:
        formats = db.query(CSVFormat).filter(CSVFormat.is_active == True).all()
        return {
            "formats": [
                {
                    "supplier_name": f.supplier_name,
                    "display_name": f.display_name,
                    "description": f.description,
                    "format_schema": f.format_schema,
                    "is_active": f.is_active
                }
                for f in formats
            ]
        }


class CSVFormatUpdate(BaseModel):
    display_name: Optional[str] = None
    description: Optional[str] = None
    format_schema: Optional[Dict] = None
    is_active: Optional[bool] = None


@app.put("/api/csv-formats/{supplier_name}")
def update_csv_format(
    supplier_name: str,
    update_data: CSVFormatUpdate,
    db: Session = Depends(get_db)
):
    """
    CSV í¬ë§· ì—…ë°ì´íŠ¸
    """
    csv_format = db.query(CSVFormat).filter(
        CSVFormat.supplier_name == supplier_name
    ).first()
    
    if not csv_format:
        raise HTTPException(status_code=404, detail=f"CSV format not found for supplier: {supplier_name}")
    
    if update_data.display_name is not None:
        csv_format.display_name = update_data.display_name
    if update_data.description is not None:
        csv_format.description = update_data.description
    if update_data.format_schema is not None:
        csv_format.format_schema = update_data.format_schema
    if update_data.is_active is not None:
        csv_format.is_active = update_data.is_active
    
    csv_format.updated_at = datetime.utcnow()
    db.commit()
    
    return {
        "message": "CSV format updated successfully",
        "supplier_name": csv_format.supplier_name,
        "display_name": csv_format.display_name
    }


@app.post("/api/csv-formats/init")
def init_csv_formats_endpoint(db: Session = Depends(get_db)):
    """
    CSV í¬ë§· ì´ˆê¸° ë°ì´í„° ìƒì„±
    """
    try:
        from .init_csv_formats import init_csv_formats
        init_csv_formats(db)
        return {"message": "CSV formats initialized successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to initialize CSV formats: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

